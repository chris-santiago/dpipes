{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dPipes - Pythonic Data Pipelines","text":""},{"location":"#about","title":"About","text":"<p><code>dPipes</code> is a Python package for creating reusable, modular, and composable data pipelines.  It's small project that came out of the desire to turn this:</p> <pre><code>import pandas as pd\ndata = (data.pipe(func_1)\n.pipe(func_2)\n.pipe(func_3)\n)\n</code></pre> <p>into this:</p> <pre><code>from dpipes.processor import PipeProcessor\nps = PipeProcessor(\nfuncs=[func_1, func_2, func_3]\n)\ndata = ps(data)\n</code></pre> <p>Now, arguably, there is not much functional difference between the two implementations. They both accomplish the same task with roughly the same amount of code. </p> <p>But, what happens if you want to apply the same pipeline of functions to a different data object?</p> <p>Using the first method, you'd need to re-write (copy/paste) your method-chaining pipeline:</p> <pre><code>new_data = (new_data.pipe(func_1)\n.pipe(func_2)\n.pipe(func_3)\n)\n</code></pre> <p>Using the latter method, you'd only need to pass in a different object to the pipeline:</p> <pre><code>new_data = ps(new_data)\n</code></pre>"},{"location":"#under-the-hood","title":"Under the Hood","text":"<p><code>dPipes</code> uses two functions from Python's <code>functools</code> module: <code>reduce</code> and <code>partial</code>. The <code>reduce</code> function enables function composition; the <code>partial</code> function enables use of arbitrary <code>kwargs</code>.</p>"},{"location":"#generalization","title":"Generalization","text":"<p>Although <code>dPipes</code> initially addressed <code>pd.DataFrame.pipe</code> method-chaining, it's extensible to any API that implements a pandas-like <code>DataFrame.pipe</code> method (e.g. Polars). Further, the  dpipes.pipeline module extends this composition to any arbitrary Python function.  </p> <p>That is, this:</p> <pre><code>result = func_3(func_2(func_1(x)))\n</code></pre> <p>or this:</p> <pre><code>result = func_1(x)\nresult = func_2(result)\nresult = func_3(result)\n</code></pre> <p>becomes this:</p> <pre><code>from dpipes.pipeline import Pipeline\npl = Pipeline(funcs=[func_1, func_2, func_3])\nresult = pl(x)\n</code></pre> <p>which is, arguably, more readable and, once again, easier to apply to other objects.</p> <p>Note</p> <p>Though the above examples use simple callables, users can pass any arbitary <code>kwargs</code> to a <code>Pipeline</code> object. The <code>PipelineProcessor</code> objects can also \"broadcast\" arguments throughout  a DataFrame pipeline.</p> <p>See the tutorials and how-to sections for additional information and examples.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#about","title":"About","text":"<p><code>dPipes</code> is a Python package for creating reusable, modular, and composable data pipelines.  It's small project that came out of the desire to turn this:</p> <pre><code>import pandas as pd\ndata = (data.pipe(func_1)\n.pipe(func_2)\n.pipe(func_3)\n)\n</code></pre> <p>into this:</p> <pre><code>from dpipes.processor import PipeProcessor\nps = PipeProcessor(\nfuncs=[func_1, func_2, func_3]\n)\ndata = ps(data)\n</code></pre> <p>Although <code>dPipes</code> initially addressed <code>pd.DataFrame.pipe</code> method-chaining, it's extensible to any API that implements a pandas-like <code>DataFrame.pipe</code> method (e.g. Polars). </p> <p>Further, the dpipes.pipeline module extends this composition to any arbitrary Python function.</p> <p>That is, this:</p> <pre><code>result = func_3(func_2(func_1(x)))\n</code></pre> <p>or this:</p> <pre><code>result = func_1(x)\nresult = func_2(result)\nresult = func_3(result)\n</code></pre> <p>becomes this:</p> <pre><code>from dpipes.pipeline import Pipeline\npl = Pipeline(funcs=[func_1, func_2, func_3])\nresult = pl(x)\n</code></pre> <p>which is, arguably, more readable and, once again, easier to apply to other objects.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p><code>dPipes</code> is can be installed via <code>pip</code>:</p> <pre><code>pip install dpipes\n</code></pre> <p>We recommend setting up a virtual environment with Python &gt;= 3.8.  </p>"},{"location":"getting-started/#benefits","title":"Benefits","text":""},{"location":"getting-started/#reusable-pipelines","title":"Reusable Pipelines","text":"<p>As you'll see in the tutorials, one of the key benefits of using <code>dPipes</code> is the reusable pipeline object that can be called on multiple datasets (provided their schemas are similar):</p> Using PipeProcessor<pre><code>for ds in [split_1, split_2, split_3]:\nresult_b = ps(ds)\npd.testing.assert_frame_equal(result_a, result_b)\n</code></pre>"},{"location":"getting-started/#modular-pipelines","title":"Modular Pipelines","text":"<p>Another is the ability to create modularized pipelines that can easily be imported and used  elsewhere in code:</p> my_module.py<pre><code>\"\"\"My pipeline module.\"\"\"\nfrom dpipes.processor import PipeProcessor\ndef task_1(...):\n...\ndef task_2(...):\n...\ndef task_3(...):\n...\ndef task_4(...):\n...\nmy_pipeline = PipeProcessor([task_1, task_2, task_3, task_4])\n</code></pre> main.py<pre><code>from my_module import my_pipeline\nmy_pipeline(my_data)\n</code></pre>"},{"location":"getting-started/#composable-pipelines","title":"Composable Pipelines","text":"<p>Finally, you can compose large, complex processing pipelines using an arbitrary number of sub-pipelines:</p> PipeProcessor Composition<pre><code>ps = PipeProcessor([\ntask_1,\ntask_2,\ntask_3,\ntask_4,\n])\ncol_ps_single = ColumnPipeProcessor(\nfuncs=[task_5, task_6],\ncols=\"customer_id\"\n)\ncol_ps_multi = ColumnPipeProcessor(\nfuncs=[task_7, task_8],\ncols=[\"customer_id\", \"invoice\"]\n)\ncol_ps_nested = ColumnPipeProcessor(\nfuncs=[task_9, task_10],\ncols=[\n[\"quantity\", \"price\"],\n[\"invoice\"],\n]\n)\npipeline = PipeProcessor([\nps,\ncol_ps_single,\ncol_ps_multi,\ncol_ps_nested,\n])\nresult = pipeline(data)\n</code></pre>"},{"location":"how-to/","title":"How To's","text":""},{"location":"pipeline-ref/","title":"Pipeline Module","text":""},{"location":"pipeline-ref/#dpipes.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Class to sequentially process an arbitrary number of functions.</p> Source code in <code>dpipes/pipeline.py</code> <pre><code>class Pipeline:\n\"\"\"\n    Class to sequentially process an arbitrary number of functions.\n    \"\"\"\ndef __init__(\nself,\nfuncs: T.Sequence[T.Callable],\nkwargs: T.Optional[\nT.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]\n] = None,\n):\n\"\"\"\n        Instantiate a Pipeline object.\n        Parameters\n        ----------\n        funcs: Sequence[Callable]\n            A sequence of callable functions.\n        kwargs: Optional[Union[Dict[str, Any], Sequence[Optional[Dict[str, Any]]]\n            An iterable collection of kwargs to apply respective functions to. If a single set of\n            kwargs is passed they will be broadcast across the sequence of functions.\n            Use `None` if a respective function does not require additional args.\n        \"\"\"\nif kwargs:\nif isinstance(kwargs, T.Sequence):\nself._check_args(funcs, kwargs)\nself.funcs = funcs\nself.kwargs = kwargs\ndef __call__(self, x):\nreducer = make_pipeline(self.funcs, self.kwargs)\nreturn reducer(x)\n@staticmethod\ndef _check_args(funcs, args):\nif len(funcs) != len(args):\nraise ValueError(\nf\"\"\"\n                    Length of `kwargs` must match length of `funcs`.\n                    Expected {len(funcs)} collections of kwargs, only got {len(args)}.\n                    \"\"\"\n)\n</code></pre>"},{"location":"pipeline-ref/#dpipes.pipeline.Pipeline.funcs","title":"<code>funcs = funcs</code>  <code>instance-attribute</code>","text":""},{"location":"pipeline-ref/#dpipes.pipeline.Pipeline.kwargs","title":"<code>kwargs = kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"pipeline-ref/#dpipes.pipeline.Pipeline.__call__","title":"<code>__call__(x)</code>","text":"Source code in <code>dpipes/pipeline.py</code> <pre><code>def __call__(self, x):\nreducer = make_pipeline(self.funcs, self.kwargs)\nreturn reducer(x)\n</code></pre>"},{"location":"pipeline-ref/#dpipes.pipeline.Pipeline.__init__","title":"<code>__init__(funcs, kwargs=None)</code>","text":"<p>Instantiate a Pipeline object.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>T.Sequence[T.Callable]</code> <p>A sequence of callable functions.</p> required <code>kwargs</code> <code>T.Optional[T.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]]</code> <p>An iterable collection of kwargs to apply respective functions to. If a single set of kwargs is passed they will be broadcast across the sequence of functions. Use <code>None</code> if a respective function does not require additional args.</p> <code>None</code> Source code in <code>dpipes/pipeline.py</code> <pre><code>def __init__(\nself,\nfuncs: T.Sequence[T.Callable],\nkwargs: T.Optional[\nT.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]\n] = None,\n):\n\"\"\"\n    Instantiate a Pipeline object.\n    Parameters\n    ----------\n    funcs: Sequence[Callable]\n        A sequence of callable functions.\n    kwargs: Optional[Union[Dict[str, Any], Sequence[Optional[Dict[str, Any]]]\n        An iterable collection of kwargs to apply respective functions to. If a single set of\n        kwargs is passed they will be broadcast across the sequence of functions.\n        Use `None` if a respective function does not require additional args.\n    \"\"\"\nif kwargs:\nif isinstance(kwargs, T.Sequence):\nself._check_args(funcs, kwargs)\nself.funcs = funcs\nself.kwargs = kwargs\n</code></pre>"},{"location":"pipeline-ref/#dpipes.pipeline.make_pipeline","title":"<code>make_pipeline(funcs, kwargs=None)</code>","text":"<p>Compose a pipeline from a sequence of functions.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>T.Sequence[T.Callable]</code> <p>A sequence of callable functions.</p> required <code>kwargs</code> <code>T.Optional[T.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]]</code> <p>An iterable collection of kwargs to apply respective functions to. If a single set of kwargs is passed they will be broadcast across the sequence of functions. Use <code>None</code> if a respective function does not require additional args.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable</code> <p>A pipeline composition function.</p> Source code in <code>dpipes/pipeline.py</code> <pre><code>def make_pipeline(\nfuncs: T.Sequence[T.Callable],\nkwargs: T.Optional[\nT.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]\n] = None,\n) -&gt; T.Callable:\n\"\"\"\n    Compose a pipeline from a sequence of functions.\n    Parameters\n    ----------\n    funcs: Sequence[Callable]\n        A sequence of callable functions.\n    kwargs: Optional[Union[Dict[str, Any], Sequence[Optional[Dict[str, Any]]]\n        An iterable collection of kwargs to apply respective functions to. If a single set of\n        kwargs is passed they will be broadcast across the sequence of functions.\n        Use `None` if a respective function does not require additional args.\n    Returns\n    -------\n    Callable\n        A pipeline composition function.\n    \"\"\"\nif kwargs:\nfuncs = make_partials(funcs, kwargs)\nreturn functools.reduce(lambda f, g: lambda x: g(f(x)), funcs)\n</code></pre>"},{"location":"pipeline-ref/#dpipes.pipeline.make_partials","title":"<code>make_partials(funcs, kwargs)</code>","text":"<p>Create a sequence of partial functions.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>T.Sequence[T.Callable]</code> <p>A sequence of callable functions.</p> required <code>kwargs</code> <code>T.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]]</code> <p>An iterable collection of kwargs to apply respective functions to. If a single set of kwargs is passed they will be broadcast across the sequence of functions. Use <code>None</code> if a respective function does not require additional args.</p> required <p>Returns:</p> Type Description <code>Sequence[Callable]</code> <p>A sequence of partial functions.</p> Source code in <code>dpipes/pipeline.py</code> <pre><code>def make_partials(\nfuncs: T.Sequence[T.Callable],\nkwargs: T.Union[T.Dict[str, T.Any], T.Sequence[T.Optional[T.Dict[str, T.Any]]]],\n) -&gt; T.Sequence[T.Callable]:\n\"\"\"\n    Create a sequence of partial functions.\n    Parameters\n    ----------\n    funcs: Sequence[Callable]\n        A sequence of callable functions.\n    kwargs: Union[Dict[str, Any], Sequence[Optional[Dict[str, Any]]\n        An iterable collection of kwargs to apply respective functions to. If a single set of\n        kwargs is passed they will be broadcast across the sequence of functions.\n        Use `None` if a respective function does not require additional args.\n    Returns\n    -------\n    Sequence[Callable]\n        A sequence of partial functions.\n    \"\"\"\nif isinstance(kwargs, dict):\n# broadcast single dict\nreturn [functools.partial(f, **kwargs) for f in funcs]\nreturn [functools.partial(f, **kw) if kw else f for f, kw in zip(funcs, kwargs)]\n</code></pre>"},{"location":"processor-ref/","title":"PipeProcessor Module","text":""},{"location":"processor-ref/#dpipes.processor.PipeProcessor","title":"<code>PipeProcessor</code>","text":"<p>         Bases: <code>Pipeline</code></p> <p>Class to sequentially process an arbitrary number of pandas.DataFrame.pipe functions.</p> Source code in <code>dpipes/processor.py</code> <pre><code>class PipeProcessor(Pipeline):\n\"\"\"\n    Class to sequentially process an arbitrary number of pandas.DataFrame.pipe functions.\n    \"\"\"\ndef __call__(self, df):\nif self.kwargs:\nself.funcs = make_partials(self.funcs, self.kwargs)\nreturn functools.reduce(lambda _df, trans: _df.pipe(trans), self.funcs, df)\n</code></pre>"},{"location":"processor-ref/#dpipes.processor.PipeProcessor.__call__","title":"<code>__call__(df)</code>","text":"Source code in <code>dpipes/processor.py</code> <pre><code>def __call__(self, df):\nif self.kwargs:\nself.funcs = make_partials(self.funcs, self.kwargs)\nreturn functools.reduce(lambda _df, trans: _df.pipe(trans), self.funcs, df)\n</code></pre>"},{"location":"processor-ref/#dpipes.processor.ColumnPipeProcessor","title":"<code>ColumnPipeProcessor</code>","text":"<p>         Bases: <code>PipeProcessor</code></p> <p>Class to sequentially process an arbitrary number of pandas.DataFrame.pipe functions by column.</p> Source code in <code>dpipes/processor.py</code> <pre><code>class ColumnPipeProcessor(PipeProcessor):\n\"\"\"\n    Class to sequentially process an arbitrary number of pandas.DataFrame.pipe functions by column.\n    \"\"\"\ndef __init__(\nself,\nfuncs: T.Sequence[T.Callable],\ncols: T.Optional[T.Union[str, T.Sequence[T.Union[str, T.Sequence[str]]]]],\n):\n\"\"\"\n        Instantiate processor.\n        Parameters\n        ----------\n        funcs: Sequence[Callable]\n            An iterable collection of user-defined functions. Function signatures should match\n            `func(df, cols)`, where `df` is a pandas.DataFrame and `cols` is an optional list of\n            columns to apply functions to.\n        cols: Optional[Union[str, Sequence[Union[str, Sequence[str]]]\n            An iterable collection of columns to apply respective functions to. If a single string\n            or single list of strings is passed they will be broadcast across the sequence of\n            functions.\n        Returns\n        -------\n        pd.DataFrame\n            A processed DataFrame.\n        \"\"\"\nsuper().__init__(funcs)\nif cols:\n# broadcast single string or single list\nif isinstance(cols, str) or (\nisinstance(cols, T.Sequence) and all(isinstance(x, str) for x in cols)\n):\nself.funcs = [functools.partial(f, cols=cols) for f in funcs]\nelse:\nself._check_args(funcs, cols)\nself.funcs = [\nfunctools.partial(f, cols=c) if c else f\nfor f, c in zip(funcs, cols)\n]\nelse:  # apply funcs to entire dataframe\nself.funcs = funcs\n</code></pre>"},{"location":"processor-ref/#dpipes.processor.ColumnPipeProcessor.funcs","title":"<code>funcs = [functools.partial(f, cols=cols) for f in funcs]</code>  <code>instance-attribute</code>","text":""},{"location":"processor-ref/#dpipes.processor.ColumnPipeProcessor.__init__","title":"<code>__init__(funcs, cols)</code>","text":"<p>Instantiate processor.</p> <p>Parameters:</p> Name Type Description Default <code>funcs</code> <code>T.Sequence[T.Callable]</code> <p>An iterable collection of user-defined functions. Function signatures should match <code>func(df, cols)</code>, where <code>df</code> is a pandas.DataFrame and <code>cols</code> is an optional list of columns to apply functions to.</p> required <code>cols</code> <code>T.Optional[T.Union[str, T.Sequence[T.Union[str, T.Sequence[str]]]]]</code> <p>An iterable collection of columns to apply respective functions to. If a single string or single list of strings is passed they will be broadcast across the sequence of functions.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A processed DataFrame.</p> Source code in <code>dpipes/processor.py</code> <pre><code>def __init__(\nself,\nfuncs: T.Sequence[T.Callable],\ncols: T.Optional[T.Union[str, T.Sequence[T.Union[str, T.Sequence[str]]]]],\n):\n\"\"\"\n    Instantiate processor.\n    Parameters\n    ----------\n    funcs: Sequence[Callable]\n        An iterable collection of user-defined functions. Function signatures should match\n        `func(df, cols)`, where `df` is a pandas.DataFrame and `cols` is an optional list of\n        columns to apply functions to.\n    cols: Optional[Union[str, Sequence[Union[str, Sequence[str]]]\n        An iterable collection of columns to apply respective functions to. If a single string\n        or single list of strings is passed they will be broadcast across the sequence of\n        functions.\n    Returns\n    -------\n    pd.DataFrame\n        A processed DataFrame.\n    \"\"\"\nsuper().__init__(funcs)\nif cols:\n# broadcast single string or single list\nif isinstance(cols, str) or (\nisinstance(cols, T.Sequence) and all(isinstance(x, str) for x in cols)\n):\nself.funcs = [functools.partial(f, cols=cols) for f in funcs]\nelse:\nself._check_args(funcs, cols)\nself.funcs = [\nfunctools.partial(f, cols=c) if c else f\nfor f, c in zip(funcs, cols)\n]\nelse:  # apply funcs to entire dataframe\nself.funcs = funcs\n</code></pre>"},{"location":"tutorial-general/","title":"General Pipeline Tutorial","text":"<p>As mentioned in the Getting Started section, the <code>dipes.pipeline</code> module extends the <code>DataFrame.pipe</code> composition into any arbitrary Python function. Let's take a look at  some examples.</p>"},{"location":"tutorial-general/#a-simple-list-example","title":"A Simple List Example","text":""},{"location":"tutorial-general/#setup","title":"Setup","text":"<p>We'll use a very simple list as our data object in this example.</p> <pre><code>import typing as T\nfrom dpipes.pipeline import Pipeline\ndata = [3, 19, 30, 18]\n</code></pre>"},{"location":"tutorial-general/#transformations","title":"Transformations","text":"<p>Let's define two simple functions: one that adds two to each element in a list, and another that  multiplies each element in a list by two.</p> Simple Transformation Functions<pre><code>def add_two(x: T.List):\nreturn [z + 2 for z in x]\ndef mult_two(x: T.List):\nreturn [z * 2 for z in x]\n</code></pre>"},{"location":"tutorial-general/#pipeline","title":"Pipeline","text":"<p>We'll create our pipeline using the <code>dpipes.pipeline.Pipeline</code> class. This class generalizes the DataFrame pipeline classes to work on any arbitrary Python object. Like the others, it accepts both a list of functions and an optional dictionary (or sequence of dictionaries) containing keyword arguments.</p> Simple Pipeline Example<pre><code>simple_pl = Pipeline([add_two, mult_two])\nresults = simple_pl(data)\nprint(results)\n</code></pre> Output<pre><code>&gt;&gt;&gt;   [10, 42, 64, 40]\n</code></pre>"},{"location":"tutorial-general/#text-preprocessing","title":"Text Preprocessing","text":"<p>In this example, we'll take a look at some basic text preprocessing tasks and demonstrate how users can construct pipelines with and without arbitrary keyword arguments.</p> <p>Note</p> <p>This tutorial requires that you have nltk installed in your Python  environment. You can install <code>nltk</code> and other tutorial dependencies by executing the command <code>pip install dpipes[demo]</code>.</p>"},{"location":"tutorial-general/#setup_1","title":"Setup","text":"<p>We'll download the relevant <code>nltk</code> objects before beginning, and create a <code>sample</code> text snippet.</p> <pre><code>import re\nimport string\nimport typing as T\nimport nltk\nfrom dpipes.pipeline import Pipeline\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nnltk.download(\"wordnet\")\nsample = \"\"\"\nHello @gabe_flomo \ud83d\udc4b\ud83c\udffe, still want us to hit that new sushi spot??? LMK when you're free cuz I \ncan't go this or next weekend since I'll be swimming!!! #sushiBros #rawFish #\ud83c\udf71\n\"\"\"\n</code></pre>"},{"location":"tutorial-general/#transformations_1","title":"Transformations","text":"<p>Next, we'll define several functions to clean and process our text to remove punctuation, lower  text case, remove stopwords, remove emoji and lemmatize tokens</p> Basic Text Preprocessing<pre><code>def remove_punctuation(tokens: T.List[str], punctuation: str) -&gt; T.List[str]:\nreturn [t for t in tokens if t not in punctuation]\ndef to_lower(tokens: T.List[str]) -&gt; T.List[str]:\nreturn [t.lower() for t in tokens]\ndef remove_stopwords(tokens: T.List[str], stopwords: T.List[str]) -&gt; T.List[str]:\nreturn [t for t in tokens if t not in stopwords]\ndef remove_emoji(text: str) -&gt; str:\nemoji_pattern = re.compile(\n\"[\"\n\"\\U0001F600-\\U0001F64F\"  # emoticons\n\"\\U0001F300-\\U0001F5FF\"  # symbols &amp; pictographs\n\"\\U0001F680-\\U0001F6FF\"  # transport &amp; map symbols\n\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n\"\\U00002702-\\U000027B0\"\n\"\\U000024C2-\\U0001F251\"\n\"]+\",\nflags=re.UNICODE,\n)\nreturn emoji_pattern.sub(r\"\", text)\ndef lemmatize(tokens: T.List[str]) -&gt; T.List[str]:\nwnl = nltk.stem.WordNetLemmatizer()\nreturn [wnl.lemmatize(t) for t in tokens]\n</code></pre>"},{"location":"tutorial-general/#pipeline_1","title":"Pipeline","text":"<p>Now, let's create our pipeline. We'll use our previously-defined functions along with <code>nltk</code>'s work tokenizer.</p> Text Preprocessing Pipeline<pre><code>ps = Pipeline(\nfuncs=[\nremove_emoji,\nnltk.tokenize.word_tokenize,\nto_lower,\nremove_punctuation,\nremove_stopwords,\nlemmatize,\n],\nkwargs=[\nNone,\nNone,\nNone,\n{\"punctuation\": string.punctuation},\n{\"stopwords\": nltk.corpus.stopwords.words(\"english\")},\nNone,\n],\n)\nresult = ps(sample)\nprint(result)\n</code></pre> Output<pre><code>&gt;&gt;&gt; ['hello', 'gabe_flomo', 'still', 'want', 'u', 'hit', 'new', 'sushi', 'spot', 'lmk', \"'re\", 'free', 'cuz', 'ca', \"n't\", 'go', 'next', 'weekend', 'since', \"'ll\", 'swimming', 'sushibros', 'rawfish']\n</code></pre>"},{"location":"tutorial-general/#composing-pipelines","title":"Composing Pipelines","text":"<p>Of course, there's nothing to say that you must include all pipeline tasks in a single, monolithic pipeline. We can breakup pipelines into logical, sub-pipelines (if so desired)-- provided that the order remains the same or is irrelevant.</p> <p>Let's decompose our original text preprocessing pipeline into two separate objects, based on the  need for keyword arguments.</p> Pipeline Composition<pre><code>sub_1 = Pipeline(\nfuncs=[\nremove_emoji,\nnltk.tokenize.word_tokenize,\nto_lower,\nlemmatize,\n]\n)\nsub_2 = Pipeline(\nfuncs=[\nremove_punctuation,\nremove_stopwords,\n],\nkwargs=[\n{\"punctuation\": string.punctuation},\n{\"stopwords\": nltk.corpus.stopwords.words(\"english\")},\n]\n)\npl = Pipeline([sub_1, sub_2])\nresult = pl(sample)\nprint(result)\n</code></pre> Output<pre><code>&gt;&gt;&gt; ['hello', 'gabe_flomo', 'still', 'want', 'u', 'hit', 'new', 'sushi', 'spot', 'lmk', \"'re\", 'free', 'cuz', 'ca', \"n't\", 'go', 'next', 'weekend', 'since', \"'ll\", 'swimming', 'sushibros', 'rawfish']\n</code></pre>"},{"location":"tutorial-general/#processing-custom-objects","title":"Processing Custom Objects","text":"<p>To again illustrate the flexibility of the <code>dpipes.Pipeline</code> class, let's create a custom dataclass for a grocery product. We'll keep it simple and track its price, description and number of available units.</p>"},{"location":"tutorial-general/#setup_2","title":"Setup","text":"<pre><code>import dataclasses\nimport typing as T\nfrom dpipes.pipeline import Pipeline\n@dataclasses.dataclass\nclass Product:\nprice: float\ndescription: str\nunits_available: float\n</code></pre>"},{"location":"tutorial-general/#transformations_2","title":"Transformations","text":"<p>Next, let's define some transformation functions. We'll create one that adjusts a product's price, one that changes its description to title case, and another that updates the number of available units.</p> Dataclass Transformations<pre><code>def adjust_price(p: Product, fn: T.Callable) -&gt; Product:\np.price = fn(p.price)\nreturn p\ndef clean_description(p: Product) -&gt; Product:\np.description = p.description.title()\nreturn p\ndef add_units(p: Product, n_units: int) -&gt; Product:\np.units_available += n_units\nreturn p\n</code></pre>"},{"location":"tutorial-general/#pipeline_2","title":"Pipeline","text":"<p>Finally, let's create a few product and process them with a pipeline.</p> Dataclass Pipeline<pre><code>eggs = Product(4.99, \"one dozen eggs\", 20)\nbread = Product(3.99, \"wheat, natural\", 10)\nmilk = Product(2.99, \"whole, 1 quart\", 20)\npl = Pipeline(\nfuncs=[\nadjust_price,\nclean_description,\nadd_units,\n],\nkwargs=[\n{\"fn\": lambda x: x * 1.1},\nNone,\n{\"n_units\": 100}\n],\n)\nfor prod in [eggs, bread, milk]:\nresult = pl(prod)\nprint(result)\n</code></pre> Output<pre><code>&gt;&gt;&gt; Product(price=5.489000000000001, description='One Dozen Eggs', units_available=120)\n&gt;&gt;&gt; Product(price=4.389, description='Wheat, Natural', units_available=110)\n&gt;&gt;&gt; Product(price=3.2890000000000006, description='Whole, 1 Quart', units_available=120)\n</code></pre>"},{"location":"tutorial-pandas/","title":"Pandas PipeProcessor Tutorial","text":""},{"location":"tutorial-pandas/#basic-example","title":"Basic Example","text":"<p>We'll be using a sample from the Online Retail II data set. It contains all the transactions occurring for a UK-based and  registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells  unique all-occasion gift-ware. </p> <p>The full dataset is available at the UCI Machine Learning Repository.</p> <p>Here's what the first few rows look like:</p> Invoice StockCode Description Quantity InvoiceDate Price Customer ID Country 489434 85048 15CM CHRISTMAS GLASS BALL 20 LIGHTS 12 2009-12-01 07:45:00 6.95 13085 United Kingdom 489434 79323P PINK CHERRY LIGHTS 12 2009-12-01 07:45:00 6.75 13085 United Kingdom 489434 79323W WHITE CHERRY LIGHTS 12 2009-12-01 07:45:00 6.75 13085 United Kingdom 489434 22041 RECORD FRAME 7\" SINGLE SIZE 48 2009-12-01 07:45:00 2.1 13085 United Kingdom 489434 21232 STRAWBERRY CERAMIC TRINKET BOX 24 2009-12-01 07:45:00 1.25 13085 United Kingdom"},{"location":"tutorial-pandas/#setup","title":"Setup","text":"<p>We'll import required packages and read in the example dataset:</p> <pre><code>import re\nimport typing as T\nimport pandas as pd\nfrom dpipes.processor import ColumnPipeProcessor, PipeProcessor\ndata = pd.read_csv(\"examples/sample.csv\")\n</code></pre>"},{"location":"tutorial-pandas/#transformations","title":"Transformations","text":"<p>Lets' define some functions to transform our data. We'll start off by converting the camel-case  column names to snake-case:</p> <pre><code>def camel_to_snake(x: str) -&gt; str:\nreturn re.sub(r\"(?&lt;!^)(?=[A-Z][a-z])\", \"_\", re.sub(r\"\\s+\", \"_\", x))\ndef clean_colnames(df: pd.DataFrame) -&gt; pd.DataFrame:\nreturn df.rename(lambda x: camel_to_snake(x).lower(), axis=1)\n</code></pre> <p>Next, we'll define a few functions that will calculate the total price per line item, calculate the total price per invoice order, calculate the number of unique products in each order, and calculate the total number of items in each order.</p> <pre><code>def add_line_total(df: pd.DataFrame) -&gt; pd.DataFrame:\nreturn df.assign(line_item_total=lambda x: x[\"quantity\"] * x[\"price\"])\ndef add_order_total(df: pd.DataFrame) -&gt; pd.DataFrame:\norder_total = (\ndf.groupby(\"invoice\")[\"line_item_total\"].sum().reset_index(name=\"order_total\")\n)\nreturn df.merge(order_total, how=\"left\", on=\"invoice\")\ndef add_order_num_products(df: pd.DataFrame) -&gt; pd.DataFrame:\nnum_products = df.groupby(\"invoice\").size().reset_index(name=\"order_num_products\")\nreturn df.merge(num_products, how=\"left\", on=\"invoice\")\ndef add_total_order_size(df: pd.DataFrame) -&gt; pd.DataFrame:\norder_size = df.groupby('invoice')['quantity'].sum().reset_index(name=\"order_size\")\nreturn df.merge(order_size, how=\"left\", on=\"invoice\")\n</code></pre>"},{"location":"tutorial-pandas/#data-pipeline","title":"Data Pipeline","text":"<p>Now, let's chain these functions together to make a simple processing pipeline.</p>"},{"location":"tutorial-pandas/#naive-version","title":"Naive Version","text":"<p>There's nothing inherently wrong about processing the data this way-- the end results will be identical to other methods. However, some software engineers do advice against over-writing an object many times.</p> Naive, Repeated Calls<pre><code>naive = clean_colnames(data)\nnaive = add_line_total(naive)\nnaive = add_order_total(naive)\nnaive = add_order_num_products(naive)\nnaive = add_total_order_size(naive)\n</code></pre>"},{"location":"tutorial-pandas/#method-chaining","title":"Method Chaining","text":"<p>A better approach is to use Pandas' <code>dataframe.pipe</code> method to chain all these operations together. As noted, you'll find the results identical.</p> Using Pandas Pipe and Method Chaining<pre><code># Method chaining\nresult_a = (\ndata.pipe(clean_colnames)\n.pipe(add_line_total)\n.pipe(add_order_total)\n.pipe(add_order_num_products)\n.pipe(add_total_order_size)\n)\npd.testing.assert_frame_equal(naive, result_a)\n</code></pre>"},{"location":"tutorial-pandas/#pipeprocessor","title":"PipeProcessor","text":"<p>Now, let's see how this looks using the <code>dpipe.PipeProcessor</code> class. We'll instantiate an object by passing a list of functions that we want to run, in order. We can this use this object to run the pipeline on any passed dataset.</p> Using PipeProcessor<pre><code>ps = PipeProcessor([\nclean_colnames,\nadd_line_total,\nadd_order_total,\nadd_order_num_products,\nadd_total_order_size\n])\nresult_b = ps(data)\npd.testing.assert_frame_equal(result_a, result_b)\n</code></pre> <p>Tip</p> <p>We're not showing it in this example, but the <code>dpipe.PipeProcessor</code> can take an optional <code>kwargs</code>  parameter, which can be a single dictionary or a list of dictionaries that map keyword arguments.  If a single dictionary is passed, those keyword arguments will be broadcast to each function in  the pipeline. If a list of dictionaries is passed, each set of keyword arguments will be applied  to their respective functions, in order.</p> <p>See <code>dpipe.PipeProcessor</code> reference for details.</p> <p>Further, we could now create modularized pipelines that can easily be imported and used elsewhere in code:</p> my_module.py<pre><code>\"\"\"My pipeline module.\"\"\"\nfrom dpipes.processor import PipeProcessor\ndef task_1(...):\n...\ndef task_2(...):\n...\ndef task_3(...):\n...\ndef task_4(...):\n...\nmy_pipeline = PipeProcessor([task_1, task_2, task_3, task_4])\n</code></pre> main.py<pre><code>from my_module import my_pipeline\nmy_pipeline(my_data)\n</code></pre>"},{"location":"tutorial-pandas/#processing-multiple-datasets","title":"Processing Multiple Datasets","text":"<p>Continuing on, imagine now that you want to run the same pipeline on multiple datasets, all with a similar schema. </p> <pre><code>split_1, split_2, split_3 = (\ndata.iloc[:300, :],\ndata.iloc[300:600, :],\ndata.iloc[600:, :],\n)\n</code></pre>"},{"location":"tutorial-pandas/#method-chaining_1","title":"Method Chaining","text":"<p>Again, the end results will be identical-- but note how you would need to re-write (or copy/paste) your entire method-chained operation to run this pipeline on new datasets.</p> Using Pandas Pipe and Method Chaining<pre><code>for ds in [split_1, split_2, split_3]:\nresult_a = (\nds.pipe(clean_colnames)\n.pipe(add_line_total)\n.pipe(add_order_total)\n.pipe(add_order_num_products)\n.pipe(add_total_order_size)\n)\n</code></pre>"},{"location":"tutorial-pandas/#pipeprocessor_1","title":"PipeProcessor","text":"<p>Contrast this with the <code>dpipes.PipeProcessor</code> methodology, where you simply need to call the original  pipeline object.</p> Using PipeProcessor<pre><code>for ds in [split_1, split_2, split_3]:\nresult_b = ps(ds)\npd.testing.assert_frame_equal(result_a, result_b)\n</code></pre>"},{"location":"tutorial-pandas/#processing-individual-columns","title":"Processing Individual Columns","text":"<p>Now, let's suppose that we want to add a few column-specific operations to our pipeline. Let's define one function where we convert floats to integers, and another were we convert integers to strings.  We'll use these functions to cast <code>customer_id</code> field as a string.</p> <pre><code>def float_to_int(\ndf: pd.DataFrame, cols: T.Union[str, T.Sequence[str]], fillna: int = -99999\n) -&gt; pd.DataFrame:\ndf[cols] = df[cols].fillna(fillna).astype(int)\nreturn df\ndef int_to_string(\ndf: pd.DataFrame, cols: T.Union[str, T.Sequence[str]]\n) -&gt; pd.DataFrame:\ndf[cols] = df[cols].astype(str)\nreturn df\n</code></pre>"},{"location":"tutorial-pandas/#method-chaining_2","title":"Method Chaining","text":"<p>Here we'll add two <code>DataFrame.pipe</code> calls with a lambda function to apply our casting operations to a specific column.</p> Adding Lambdas to the Method Chain<pre><code>result_a = (\ndata.pipe(clean_colnames)\n.pipe(add_line_total)\n.pipe(add_order_total)\n.pipe(add_order_num_products)\n.pipe(add_total_order_size)\n.pipe(lambda x: float_to_int(x, \"customer_id\"))\n.pipe(lambda x: int_to_string(x, \"customer_id\"))\n)\n</code></pre>"},{"location":"tutorial-pandas/#pipeprocessor_2","title":"PipeProcessor","text":"<p>We'll create two new <code>PipeProcessor</code> objects: one to process functions on the <code>customer_id</code> function, and another that will compose both our original and column-specific pipelines into a single processor.</p> <p>One can easily create an arbitrary number of sub-pipelines and pipeline compositions.</p> PipeProcessor Composition<pre><code>col_ps = PipeProcessor([float_to_int, int_to_string], {\"cols\": \"customer_id\"})\npipeline = PipeProcessor([ps, col_ps])\nresult_b = pipeline(data)\npd.testing.assert_frame_equal(result_a, result_b)\n</code></pre> <p>Note</p> <p>Note that we only passed a single dictionary to the <code>dpipes.PipeProcessor</code> constructor, and it broadcast those keyword arguments to both functions within the pipeline.</p> <p>Although both methods produce identical results, only the use of <code>PipeProcessor</code> provides a reusable, modular pipeline object.</p>"},{"location":"tutorial-pandas/#columnpipeprocessor","title":"ColumnPipeProcessor","text":"<p>Finally, if the only keyword arguments to our transformation functions are column names, we can  choose to use the <code>dpipes.ColumnPipeProcessor</code>,  instead. Similar to the <code>dpipes.PipeProcessor</code> class, we can pass in a single column or single list of columns to broadcast to the functions within the pipeline. You can also specify specific column(s) for each function to act on by passing a list of lists.</p> ColumnPipeProcessor<pre><code>col_ps = ColumnPipeProcessor([float_to_int, int_to_string], cols=\"customer_id\")\npipeline = PipeProcessor([ps, col_ps])\nresult_b = pipeline(data)\npd.testing.assert_frame_equal(result_a, result_b)\n</code></pre>"},{"location":"tutorial-polars/","title":"Polars PipeProcessor Tutorial","text":"<p>Note</p> <p>This tutorial assumes that you have Polars installed in your Python  environment. You can install <code>Polars</code> and other tutorial dependencies by executing the command <code>pip install dpipes[demo]</code>.</p> <p>As mentioned in Getting Started, <code>dpipes.PipeProcessor</code> is extensible to any  API that implements a Pandas-like <code>DataFrame.pipe</code> method.</p> <p>Tip</p> <p><code>dPipes</code> is extensible beyond Pandas-like APIs, too. The <code>dpipes.Pipeline</code> module generalizes the pipeline composability to any arbitrary Python functions and objects.</p> <p>We'll run through a condensed version Pandas tutorial, using Polars as the swap from Pandas to Polars is 1:1. The only changes occur in the transformation functions--  Polars API is much more concise than Pandas.</p>"},{"location":"tutorial-polars/#basic-example","title":"Basic Example","text":"<p>We'll be using a sample from the Online Retail II data set. It contains all the transactions occurring for a UK-based and  registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells  unique all-occasion gift-ware. </p> <p>The full dataset is available at the UCI Machine Learning Repository.</p> <p>Here's what the first few rows look like:</p> Invoice StockCode Description Quantity InvoiceDate Price Customer ID Country 489434 85048 15CM CHRISTMAS GLASS BALL 20 LIGHTS 12 2009-12-01 07:45:00 6.95 13085 United Kingdom 489434 79323P PINK CHERRY LIGHTS 12 2009-12-01 07:45:00 6.75 13085 United Kingdom 489434 79323W WHITE CHERRY LIGHTS 12 2009-12-01 07:45:00 6.75 13085 United Kingdom 489434 22041 RECORD FRAME 7\" SINGLE SIZE 48 2009-12-01 07:45:00 2.1 13085 United Kingdom 489434 21232 STRAWBERRY CERAMIC TRINKET BOX 24 2009-12-01 07:45:00 1.25 13085 United Kingdom"},{"location":"tutorial-polars/#setup","title":"Setup","text":"<p>We'll import required packages and read in the example dataset:</p> <pre><code>import re\nimport typing as T\nimport polars as pl\nfrom polars import testing\nfrom dpipes.processor import ColumnPipeProcessor, PipeProcessor\ndata = pl.read_csv(\"examples/sample.csv\", ignore_errors=True)\n</code></pre>"},{"location":"tutorial-polars/#transformations","title":"Transformations","text":"<p>Lets' define some functions to transform our data. We'll start off by converting the camel-case  column names to snake-case:</p> <pre><code>def camel_to_snake(x: str) -&gt; str:\nreturn re.sub(r\"(?&lt;!^)(?=[A-Z][a-z])\", \"_\", re.sub(r\"\\s+\", \"_\", x))\ndef clean_colnames(df: pl.DataFrame) -&gt; pl.DataFrame:\nreturn df.rename({x: camel_to_snake(x).lower() for x in df.columns})\n</code></pre> <p>Next, we'll define a few functions that will calculate the total price per line item, calculate the total price per invoice order, calculate the number of unique products in each order, and calculate the total number of items in each order.</p> <p>Finally, let's define one function where we convert floats to integers, and another were we convert  integers to strings.  We'll use these functions to cast <code>customer_id</code> field as a string.</p> Transformation Functions with Polars<pre><code>def add_line_total(df: pl.DataFrame) -&gt; pl.DataFrame:\nreturn df.with_columns(\nline_item_total=pl.col(\"quantity\") * pl.col(\"price\")\n)\ndef add_order_total(df: pl.DataFrame) -&gt; pl.DataFrame:\nreturn df.with_columns(\norder_total=pl.col(\"line_item_total\").sum().over(\"invoice\")\n)\ndef add_order_num_products(df: pl.DataFrame) -&gt; pl.DataFrame:\nreturn df.with_columns(\norder_size=pl.count().over(\"invoice\")\n)\ndef add_total_order_size(df: pl.DataFrame) -&gt; pl.DataFrame:\nreturn df.with_columns(\norder_size=pl.col(\"quantity\").sum().over(\"invoice\")\n)\ndef float_to_int(\ndf: pl.DataFrame, cols: T.Union[str, T.Sequence[str]], fillna: int = -99999\n) -&gt; pl.DataFrame:\nreturn df.with_columns(pl.col(cols).fill_nan(fillna).cast(int))\ndef int_to_string(\ndf: pl.DataFrame, cols: T.Union[str, T.Sequence[str]]\n) -&gt; pl.DataFrame:\nreturn df.with_columns(pl.col(cols).cast(str))\n</code></pre>"},{"location":"tutorial-polars/#data-pipeline","title":"Data Pipeline","text":"<p>Now, let's chain these functions together to make a simple processing pipeline.</p>"},{"location":"tutorial-polars/#method-chaining","title":"Method Chaining","text":"<p>We can use Polars' <code>dataframe.pipe</code> method to chain all these operations together. We'll add two  <code>DataFrame.pipe</code> calls with a lambda function to apply our casting operations to a specific column.</p> Using Polars Pipe and Method Chaining<pre><code>result_a = (\ndata.pipe(clean_colnames)\n.pipe(add_line_total)\n.pipe(add_order_total)\n.pipe(add_order_num_products)\n.pipe(add_total_order_size)\n.pipe(lambda x: float_to_int(x, \"customer_id\"))\n.pipe(lambda x: int_to_string(x, \"customer_id\"))\n)\n</code></pre>"},{"location":"tutorial-polars/#pipeprocessor","title":"PipeProcessor","text":"<p>Now, let's see how this looks using the <code>dpipe.PipeProcessor</code> class. We'll instantiate an object by passing a list of functions that we want to run, in order. We can this use this object to run the pipeline on any passed dataset.</p> <p>We'll create two new <code>PipeProcessor</code> objects: one to process functions on the <code>customer_id</code> function, and another that will compose both our original and column-specific pipelines into a single processor.</p> <p>One can easily create an arbitrary number of sub-pipelines and pipeline compositions.</p> PipeProcessor Composition<pre><code>ps = PipeProcessor([\nclean_colnames,\nadd_line_total,\nadd_order_total,\nadd_order_num_products,\nadd_total_order_size\n])\ncol_ps = PipeProcessor([float_to_int, int_to_string], {\"cols\": \"customer_id\"})\npipeline = PipeProcessor([ps, col_ps])\nresult_b = pipeline(data)\ntesting.assert_frame_equal(result_a, result_b)\n</code></pre> <p>Note</p> <p>Note that we only passed a single dictionary to the <code>dpipes.PipeProcessor</code> constructor, and it broadcast those keyword arguments to both functions within the pipeline.</p> <p>Although both methods produce identical results, only the use of <code>PipeProcessor</code> provides a reusable, modular pipeline object.</p>"},{"location":"tutorial-polars/#columnpipeprocessor","title":"ColumnPipeProcessor","text":"<p>Finally, if the only keyword arguments to our transformation functions are column names, we can  choose to use the <code>dpipes.ColumnPipeProcessor</code>,  instead. Similar to the <code>dpipes.PipeProcessor</code> class, we can pass in a single column or single list of columns to broadcast to the functions within the pipeline. You can also specify specific column(s) for each function to act on by passing a list of lists.</p> ColumnPipeProcessor<pre><code>col_ps = ColumnPipeProcessor([float_to_int, int_to_string], cols=\"customer_id\")\npipeline = PipeProcessor([ps, col_ps])\nresult_b = pipeline(data)\ntesting.assert_frame_equal(result_a, result_b)\n</code></pre>"}]}